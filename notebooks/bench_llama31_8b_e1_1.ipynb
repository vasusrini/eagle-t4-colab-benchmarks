{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh_eFlSURZ1I"
      },
      "outputs": [],
      "source": [
        "# File        : bench_llama31_8b_e1_1.ipynb\n",
        "# Base Model  : meta-llama/Llama-3.1-8B-Instruct\n",
        "# EAGLE Model : yuhuili/EAGLE-LLaMA3.1-Instruct-8B\n",
        "# EAGLE Type  : EAGLE-1 (use_eagle3=False)\n",
        "# Quantization: 8-bit (BitsAndBytes)\n",
        "# Target GPU  : T4 (15GB VRAM)\n",
        "# Est. VRAM   :\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import pandas as pd\n",
        "import IPython\n",
        "from google.colab import userdata\n",
        "\n",
        "def clear_vram():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ EAGLE BENCHMARK - LLaMA3.1 8B | EAGLE-1 | 8-bit | #1\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "    from huggingface_hub import login\n",
        "    login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=False)\n",
        "except: pass\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Environment Setup\n",
        "# Use IPython %pip magic so packages are immediately available to the kernel.\n",
        "# transformers==4.53.1 is required for EAGLE compatibility.\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüì¶ Setting up environment...\")\n",
        "IPython.get_ipython().run_line_magic('pip', 'uninstall -y transformers bitsandbytes accelerate -q')\n",
        "IPython.get_ipython().run_line_magic('pip', 'install transformers==4.53.1 bitsandbytes>=0.44.0 accelerate==0.26.0 -q')\n",
        "\n",
        "# Force Python to reload the newly installed packages by purging cached modules\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if 'transformers' in mod or 'huggingface_hub' in mod:\n",
        "        del sys.modules[mod]\n",
        "\n",
        "if not os.path.exists(\"EAGLE\"):\n",
        "    subprocess.run(\"git clone -q https://github.com/SafeAILab/EAGLE.git\", shell=True, check=False)\n",
        "sys.path.insert(0, os.path.abspath(\"EAGLE\"))\n",
        "\n",
        "# Patch cnets\n",
        "cnets = \"EAGLE/eagle/model/cnets.py\"\n",
        "if os.path.exists(cnets):\n",
        "    with open(cnets, 'r') as f: c = f.read()\n",
        "    if \"getattr(config, 'draft_vocab_size'\" not in c:\n",
        "        c = c.replace(\"self.lm_head=nn.Linear(config.hidden_size,config.draft_vocab_size,bias=False)\", \"draft_vocab_size = getattr(config, 'draft_vocab_size', config.vocab_size)\\n        self.lm_head=nn.Linear(config.hidden_size,draft_vocab_size,bias=False)\")\n",
        "        with open(cnets, 'w') as f: f.write(c)\n",
        "\n",
        "# Monkey-patch to fix additional_chat_templates 404 bug.\n",
        "# transformers==4.53.1 calls list_repo_templates() which throws 404 for older\n",
        "# model repos without an additional_chat_templates folder. Patching both the\n",
        "# source module and the already-imported reference in tokenization_utils_base.\n",
        "import transformers.utils.hub as _hub\n",
        "import transformers.tokenization_utils_base as _tub\n",
        "_hub.list_repo_templates = lambda repo_id, **kwargs: []\n",
        "_tub.list_repo_templates = lambda repo_id, **kwargs: []\n",
        "\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "from eagle.model.ea_model import EaModel\n",
        "\n",
        "\n",
        "BASE_ID  = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "EAGLE_ID = \"yuhuili/EAGLE-LLaMA3.1-Instruct-8B\"\n",
        "\n",
        "# Use longer prompt for better EAGLE performance\n",
        "PROMPT = \"Write a detailed explanation about how machine learning models work, including the training process, inference, and optimization techniques.\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_ID, trust_remote_code=True)\n",
        "messages = [{\"role\": \"user\", \"content\": PROMPT}]\n",
        "chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(chat, return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[1]\n",
        "\n",
        "results = []\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 1: BASELINE\n",
        "# =============================================================================\n",
        "clear_vram()\n",
        "print(\"\\n‚öôÔ∏è  Testing [1/2]: Baseline...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Proper warmup\n",
        "with torch.no_grad():\n",
        "    for _ in range(2):\n",
        "        _ = model.generate(inputs.input_ids, max_new_tokens=20, do_sample=False)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Benchmark with EXACTLY 100 tokens\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(inputs.input_ids, max_new_tokens=100, do_sample=False)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - t1\n",
        "\n",
        "tokens = out.shape[1] - input_len\n",
        "tps = tokens / elapsed\n",
        "results.append({\n",
        "    \"Method\": \"Baseline\",\n",
        "    \"TPS\": tps,\n",
        "    \"Time\": elapsed,\n",
        "    \"Tokens\": tokens,\n",
        "    \"Input\": input_len\n",
        "})\n",
        "print(f\"   Generated: {tokens} tokens in {elapsed:.2f}s = {tps:.2f} tok/s\")\n",
        "\n",
        "del model\n",
        "clear_vram()\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 2: EAGLE-1\n",
        "# =============================================================================\n",
        "print(\"\\nü¶Ö Testing [2/2]: EAGLE-1...\")\n",
        "eagle = EaModel.from_pretrained(\n",
        "    base_model_path=BASE_ID,\n",
        "    ea_model_path=EAGLE_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_eagle3=False,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "eagle.eval()\n",
        "\n",
        "# Proper warmup\n",
        "with torch.no_grad():\n",
        "    for _ in range(2):\n",
        "        _ = eagle.eagenerate(inputs.input_ids, max_new_tokens=20)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Benchmark with EXACTLY 100 tokens\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = eagle.eagenerate(inputs.input_ids, max_new_tokens=100, temperature=0.5)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - t1\n",
        "\n",
        "tokens = out.shape[1] - input_len\n",
        "tps = tokens / elapsed\n",
        "results.append({\n",
        "    \"Method\": \"EAGLE-1\",\n",
        "    \"TPS\": tps,\n",
        "    \"Time\": elapsed,\n",
        "    \"Tokens\": tokens,\n",
        "    \"Input\": input_len\n",
        "})\n",
        "print(f\"   Generated: {tokens} tokens in {elapsed:.2f}s = {tps:.2f} tok/s\")\n",
        "\n",
        "del eagle\n",
        "clear_vram()\n",
        "\n",
        "# =============================================================================\n",
        "# RESULTS\n",
        "# =============================================================================\n",
        "df = pd.DataFrame(results)\n",
        "df['Speedup'] = df['TPS'] / df.iloc[0]['TPS']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä BENCHMARK RESULTS (100 tokens, longer prompt)\")\n",
        "print(\"=\"*70)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "baseline_tps = df.iloc[0]['TPS']\n",
        "eagle_tps = df.iloc[1]['TPS']\n",
        "speedup = df.iloc[1]['Speedup']\n",
        "time_saved = df.iloc[0]['Time'] - df.iloc[1]['Time']\n",
        "\n",
        "print(f\"\\nüéØ Results:\")\n",
        "print(f\"   Baseline: {baseline_tps:.2f} tok/s\")\n",
        "print(f\"   EAGLE-1:  {eagle_tps:.2f} tok/s\")\n",
        "print(f\"   Speedup:  {speedup:.2f}x\")\n",
        "print(f\"   Time saved: {time_saved:.2f}s\")\n",
        "\n",
        "if speedup >= 1.3:\n",
        "    print(\"\\n‚úÖ EXCELLENT: EAGLE-1 provides significant speedup\")\n",
        "elif speedup >= 1.15:\n",
        "    print(\"\\n‚úì GOOD: EAGLE-1 provides moderate speedup\")\n",
        "elif speedup >= 1.05:\n",
        "    print(\"\\n‚ö†Ô∏è MODEST: EAGLE-1 provides minimal speedup\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ISSUE: EAGLE-1 is slower than baseline\")\n",
        "    print(\"   Possible causes:\")\n",
        "    print(\"   - Prompt too short (EAGLE needs longer context)\")\n",
        "    print(\"   - Generation too short (overhead dominates)\")\n",
        "    print(\"   - Quantization degrading draft quality too much\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DIVERGENCE ANALYSIS\n",
        "# Derives Total Variation Distance (TVD) from the observed speedup.\n",
        "#\n",
        "# Theory:\n",
        "#   In speculative decoding, if the draft model accepts tokens with\n",
        "#   probability alpha, the expected accepted tokens per step follows:\n",
        "#       tau = (1 - alpha^(gamma+1)) / (1 - alpha)\n",
        "#   where gamma is the draft length (5 for EAGLE-1).\n",
        "#   TVD is then: TVD = 1 - alpha\n",
        "#   A lower TVD means the draft distribution closely matches the target.\n",
        "# =============================================================================\n",
        "gamma    = 5    # EAGLE-1 draft length (tokens proposed per step)\n",
        "overhead = 0.1  # EAGLE head forward pass overhead (~10% of target model)\n",
        "tau      = speedup * (1 + overhead)  # Estimated avg accepted tokens/step\n",
        "\n",
        "\n",
        "def estimate_alpha(target_tau, g, tolerance=0.001):\n",
        "    \"\"\"Binary search for acceptance rate alpha given observed tau.\"\"\"\n",
        "    low, high = 0.0, 1.0\n",
        "    for _ in range(20):\n",
        "        mid = (low + high) / 2\n",
        "        current_tau = (1 - mid**(g + 1)) / (1 - mid) if mid < 1.0 else g + 1\n",
        "        if abs(current_tau - target_tau) < tolerance:\n",
        "            return mid\n",
        "        if current_tau < target_tau:\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid\n",
        "    return low\n",
        "\n",
        "\n",
        "alpha = estimate_alpha(tau, gamma)\n",
        "tvd   = 1.0 - alpha\n",
        "\n",
        "print(\"\\nüìê Divergence Analysis:\")\n",
        "print(f\"   Avg Tokens Accepted/Step (œÑ) : {tau:.2f}\")\n",
        "print(f\"   Token Acceptance Rate (Œ±)    : {alpha*100:.1f}%\")\n",
        "print(f\"   Total Variation Distance (TVD): {tvd:.4f}\")\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "if tvd < 0.2:\n",
        "    print(f\"   EXCELLENT ALIGNMENT (TVD={tvd:.2f}): Draft head closely mirrors\")\n",
        "    print(f\"   the target distribution ‚Äî high acceptance drives the speedup.\")\n",
        "elif tvd < 0.4:\n",
        "    print(f\"   MODERATE ALIGNMENT (TVD={tvd:.2f}): Draft accepts most tokens\")\n",
        "    print(f\"   but diverges on less predictable outputs.\")\n",
        "else:\n",
        "    print(f\"   DIVERGENT (TVD={tvd:.2f}): Draft and target disagree frequently.\")\n",
        "    print(f\"   Consider a longer prompt or more generation steps.\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    }
  ]
}