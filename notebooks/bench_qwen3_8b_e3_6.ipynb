{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# File        : bench_qwen3_8b_e3_6.ipynb\n",
        "# Base Model  : Qwen/Qwen3-8B\n",
        "# EAGLE Model : AngelSlim/Qwen3-8B_eagle3\n",
        "# EAGLE Type  : EAGLE-3 (use_eagle3=True)\n",
        "# Quantization: 8-bit (BitsAndBytes)\n",
        "# Target GPU  : T4 (15GB VRAM)\n",
        "# Est. VRAM   :\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "\n",
        "def clear_vram():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ EAGLE BENCHMARK - Qwen3 8B | EAGLE-3 | 8-bit | #6\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "    from huggingface_hub import login\n",
        "    login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=False)\n",
        "except: pass\n",
        "\n",
        "print(\"\\nüì¶ Setting up environment...\")\n",
        "subprocess.run(\"pip uninstall -y transformers bitsandbytes accelerate -q\", shell=True, check=False)\n",
        "subprocess.run(\"pip install transformers==4.53.1 bitsandbytes>=0.44.0 accelerate==0.26.0 -q\", shell=True, check=False)\n",
        "\n",
        "import shutil\n",
        "if os.path.exists(\"EAGLE\"):\n",
        "    shutil.rmtree(\"EAGLE\")\n",
        "subprocess.run(\"git clone -q https://github.com/SafeAILab/EAGLE.git\", shell=True, check=False)\n",
        "sys.path.insert(0, os.path.abspath(\"EAGLE\"))\n",
        "\n",
        "# Patch cnets\n",
        "cnets = \"EAGLE/eagle/model/cnets.py\"\n",
        "if os.path.exists(cnets):\n",
        "    with open(cnets, 'r') as f: c = f.read()\n",
        "    if \"getattr(config, 'draft_vocab_size'\" not in c:\n",
        "        c = c.replace(\"self.lm_head=nn.Linear(config.hidden_size,config.draft_vocab_size,bias=False)\", \"draft_vocab_size = getattr(config, 'draft_vocab_size', config.vocab_size)\\n        self.lm_head=nn.Linear(config.hidden_size,draft_vocab_size,bias=False)\")\n",
        "        with open(cnets, 'w') as f: f.write(c)\n",
        "\n",
        "# NOTE: No Qwen3 stub patch here - we need real Qwen3 support for this benchmark\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "from eagle.model.ea_model import EaModel\n",
        "\n",
        "BASE_ID = \"Qwen/Qwen3-8B\"\n",
        "EAGLE_ID = \"AngelSlim/Qwen3-8B_eagle3\"\n",
        "\n",
        "PROMPT = \"Write a detailed explanation about how machine learning models work, including the training process, inference, and optimization techniques.\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "from transformers import Qwen2Tokenizer\n",
        "tokenizer = Qwen2Tokenizer.from_pretrained(BASE_ID, trust_remote_code=True)\n",
        "messages = [{\"role\": \"user\", \"content\": PROMPT}]\n",
        "chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(chat, return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[1]\n",
        "\n",
        "results = []\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 1: BASELINE\n",
        "# =============================================================================\n",
        "clear_vram()\n",
        "print(\"\\n‚öôÔ∏è  Testing [1/2]: Baseline...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(2):\n",
        "        _ = model.generate(inputs.input_ids, max_new_tokens=20, do_sample=False)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(inputs.input_ids, max_new_tokens=100, do_sample=False)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - t1\n",
        "\n",
        "tokens = out.shape[1] - input_len\n",
        "tps = tokens / elapsed\n",
        "results.append({\"Method\": \"Baseline\", \"TPS\": tps, \"Time\": elapsed, \"Tokens\": tokens, \"Input\": input_len})\n",
        "print(f\"   Generated: {tokens} tokens in {elapsed:.2f}s = {tps:.2f} tok/s\")\n",
        "\n",
        "del model\n",
        "clear_vram()\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 2: EAGLE\n",
        "# =============================================================================\n",
        "print(\"\\nü¶Ö Testing [2/2]: EAGLE...\")\n",
        "eagle = EaModel.from_pretrained(\n",
        "    base_model_path=BASE_ID,\n",
        "    ea_model_path=EAGLE_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_eagle3=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "eagle.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(2):\n",
        "        _ = eagle.eagenerate(inputs.input_ids, max_new_tokens=20)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = eagle.eagenerate(inputs.input_ids, max_new_tokens=100, temperature=0.5)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - t1\n",
        "\n",
        "tokens = out.shape[1] - input_len\n",
        "tps = tokens / elapsed\n",
        "results.append({\"Method\": \"EAGLE\", \"TPS\": tps, \"Time\": elapsed, \"Tokens\": tokens, \"Input\": input_len})\n",
        "print(f\"   Generated: {tokens} tokens in {elapsed:.2f}s = {tps:.2f} tok/s\")\n",
        "\n",
        "del eagle\n",
        "clear_vram()\n",
        "\n",
        "# =============================================================================\n",
        "# RESULTS\n",
        "# =============================================================================\n",
        "df = pd.DataFrame(results)\n",
        "df['Speedup'] = df['TPS'] / df.iloc[0]['TPS']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä BENCHMARK RESULTS (100 tokens, longer prompt)\")\n",
        "print(\"=\"*70)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "baseline_tps = df.iloc[0]['TPS']\n",
        "eagle_tps = df.iloc[1]['TPS']\n",
        "speedup = df.iloc[1]['Speedup']\n",
        "time_saved = df.iloc[0]['Time'] - df.iloc[1]['Time']\n",
        "\n",
        "print(f\"\\nüéØ Results:\")\n",
        "print(f\"   Baseline: {baseline_tps:.2f} tok/s\")\n",
        "print(f\"   EAGLE:    {eagle_tps:.2f} tok/s\")\n",
        "print(f\"   Speedup:  {speedup:.2f}x\")\n",
        "print(f\"   Time saved: {time_saved:.2f}s\")\n",
        "\n",
        "if speedup >= 1.3:\n",
        "    print(\"\\n‚úÖ EXCELLENT: EAGLE provides significant speedup\")\n",
        "elif speedup >= 1.15:\n",
        "    print(\"\\n‚úì GOOD: EAGLE provides moderate speedup\")\n",
        "elif speedup >= 1.05:\n",
        "    print(\"\\n‚ö†Ô∏è MODEST: EAGLE provides minimal speedup\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ISSUE: EAGLE is slower than baseline\")\n",
        "    print(\"   Possible causes:\")\n",
        "    print(\"   - Prompt too short (EAGLE needs longer context)\")\n",
        "    print(\"   - Generation too short (overhead dominates)\")\n",
        "    print(\"   - Quantization degrading draft quality too much\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DIVERGENCE ANALYSIS\n",
        "# Derives Total Variation Distance (TVD) from the observed speedup.\n",
        "#\n",
        "# Theory:\n",
        "#   In speculative decoding, if the draft model accepts tokens with\n",
        "#   probability alpha, the expected accepted tokens per step follows:\n",
        "#       tau = (1 - alpha^(gamma+1)) / (1 - alpha)\n",
        "#   where gamma is the draft length (5 for EAGLE-3).\n",
        "#   TVD is then: TVD = 1 - alpha\n",
        "#   A lower TVD means the draft distribution closely matches the target.\n",
        "# =============================================================================\n",
        "gamma    = 5    # EAGLE-3 draft length (tokens proposed per step)\n",
        "overhead = 0.1  # EAGLE head forward pass overhead (~10% of target model)\n",
        "tau      = speedup * (1 + overhead)  # Estimated avg accepted tokens/step\n",
        "\n",
        "\n",
        "def estimate_alpha(target_tau, g, tolerance=0.001):\n",
        "    \"\"\"Binary search for acceptance rate alpha given observed tau.\"\"\"\n",
        "    low, high = 0.0, 1.0\n",
        "    for _ in range(20):\n",
        "        mid = (low + high) / 2\n",
        "        current_tau = (1 - mid**(g + 1)) / (1 - mid) if mid < 1.0 else g + 1\n",
        "        if abs(current_tau - target_tau) < tolerance:\n",
        "            return mid\n",
        "        if current_tau < target_tau:\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid\n",
        "    return low\n",
        "\n",
        "\n",
        "alpha = estimate_alpha(tau, gamma)\n",
        "tvd   = 1.0 - alpha\n",
        "\n",
        "print(\"\\nüìê Divergence Analysis:\")\n",
        "print(f\"   Avg Tokens Accepted/Step (œÑ) : {tau:.2f}\")\n",
        "print(f\"   Token Acceptance Rate (Œ±)    : {alpha*100:.1f}%\")\n",
        "print(f\"   Total Variation Distance (TVD): {tvd:.4f}\")\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "if tvd < 0.2:\n",
        "    print(f\"   EXCELLENT ALIGNMENT (TVD={tvd:.2f}): Draft head closely mirrors\")\n",
        "    print(f\"   the target distribution ‚Äî high acceptance drives the speedup.\")\n",
        "elif tvd < 0.4:\n",
        "    print(f\"   MODERATE ALIGNMENT (TVD={tvd:.2f}): Draft accepts most tokens\")\n",
        "    print(f\"   but diverges on less predictable outputs.\")\n",
        "else:\n",
        "    print(f\"   DIVERGENT (TVD={tvd:.2f}): Draft and target disagree frequently.\")\n",
        "    print(f\"   Consider a longer prompt or more generation steps.\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "4UkY_GFa1yf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}