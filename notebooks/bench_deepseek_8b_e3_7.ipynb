{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# File        : bench_deepseek_8b_e3_7.ipynb\n",
        "# Author      : EAGLE Benchmark Suite\n",
        "# Description : Benchmark EAGLE-3 speculative decoding on DeepSeek-R1-Distill-Llama-8B\n",
        "#               Compares baseline autoregressive generation vs EAGLE-3 speculative\n",
        "#               decoding and computes TVD-based divergence metrics.\n",
        "#\n",
        "# Base Model  : deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
        "# EAGLE Model : yuhuili/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B\n",
        "# EAGLE Type  : EAGLE-3 (use_eagle3=True)\n",
        "# Quantization: 8-bit (BitsAndBytes)\n",
        "# Target GPU  : T4 (15GB VRAM)\n",
        "# Est. VRAM   : ~9.5 GB base + ~0.5 GB EAGLE head + ~2-3 GB KV cache\n",
        "#\n",
        "# Dependencies:\n",
        "#   - transformers==4.53.1\n",
        "#   - bitsandbytes>=0.44.0\n",
        "#   - accelerate==0.26.0\n",
        "#   - huggingface_hub==0.23.0\n",
        "#   - EAGLE repo: https://github.com/SafeAILab/EAGLE\n",
        "#\n",
        "# Notes:\n",
        "#   - Requires HF_TOKEN secret in Colab for model access\n",
        "#   - Monkey-patches transformers hub to fix additional_chat_templates 404 bug\n",
        "#   - Qwen3 stub patch not needed (DeepSeek uses LLaMA architecture)\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import IPython\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Utility: Free GPU memory between tests\n",
        "# -----------------------------------------------------------------------------\n",
        "def clear_vram():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Header\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ EAGLE BENCHMARK - DeepSeek-R1 8B | EAGLE-3 | 8-bit | #7\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Authentication\n",
        "# -----------------------------------------------------------------------------\n",
        "try:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "    from huggingface_hub import login\n",
        "    login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Environment Setup\n",
        "# Uninstall and reinstall pinned versions for reproducibility.\n",
        "# huggingface_hub==0.23.0 is required for HF_HUB_ENABLE_HF_TRANSFER compat.\n",
        "# transformers==4.53.1 is required for EAGLE compatibility.\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüì¶ Setting up environment...\")\n",
        "IPython.get_ipython().run_line_magic('pip', 'uninstall -y transformers bitsandbytes accelerate -q')\n",
        "IPython.get_ipython().run_line_magic('pip', 'install transformers==4.53.1 bitsandbytes>=0.44.0 accelerate==0.26.0 -q')\n",
        "\n",
        "# Force Python to reload the newly installed packages by purging cached modules\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if 'transformers' in mod or 'huggingface_hub' in mod:\n",
        "        del sys.modules[mod]\n",
        "\n",
        "# Monkey-patch to fix additional_chat_templates 404 bug.\n",
        "# transformers==4.53.1 calls list_repo_templates() which throws 404 for older\n",
        "# model repos without an additional_chat_templates folder. Patching both the\n",
        "# source module and the already-imported reference in tokenization_utils_base.\n",
        "import transformers.utils.hub as _hub\n",
        "import transformers.tokenization_utils_base as _tub\n",
        "_hub.list_repo_templates = lambda repo_id, **kwargs: []\n",
        "_tub.list_repo_templates = lambda repo_id, **kwargs: []\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Clone and Patch EAGLE Repository\n",
        "# Always do a fresh clone to ensure patches apply cleanly.\n",
        "# -----------------------------------------------------------------------------\n",
        "if os.path.exists(\"EAGLE\"):\n",
        "    shutil.rmtree(\"EAGLE\")\n",
        "subprocess.run(\"git clone -q https://github.com/SafeAILab/EAGLE.git\",\n",
        "               shell=True, check=False)\n",
        "sys.path.insert(0, os.path.abspath(\"EAGLE\"))\n",
        "\n",
        "# Patch 1: cnets.py - some EAGLE weight configs omit draft_vocab_size,\n",
        "# falling back to the base model's vocab_size is safe and correct.\n",
        "cnets = \"EAGLE/eagle/model/cnets.py\"\n",
        "if os.path.exists(cnets):\n",
        "    with open(cnets, 'r') as f:\n",
        "        c = f.read()\n",
        "    if \"getattr(config, 'draft_vocab_size'\" not in c:\n",
        "        c = c.replace(\n",
        "            \"self.lm_head=nn.Linear(config.hidden_size,config.draft_vocab_size,bias=False)\",\n",
        "            \"draft_vocab_size = getattr(config, 'draft_vocab_size', config.vocab_size)\\n\"\n",
        "            \"        self.lm_head=nn.Linear(config.hidden_size,draft_vocab_size,bias=False)\"\n",
        "        )\n",
        "        with open(cnets, 'w') as f:\n",
        "            f.write(c)\n",
        "\n",
        "# Patch 2: Stub out Qwen3 KV module - it imports transformers internals\n",
        "# that don't exist in 4.53.1. Not needed for LLaMA/DeepSeek models.\n",
        "qwen3_kv = \"EAGLE/eagle/model/modeling_qwen3_kv.py\"\n",
        "if os.path.exists(qwen3_kv):\n",
        "    with open(qwen3_kv, 'w') as f:\n",
        "        f.write(\"from torch import nn\\nclass Qwen3ForCausalLM(nn.Module): pass\\n\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Imports\n",
        "# -----------------------------------------------------------------------------\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, PreTrainedTokenizerFast\n",
        "from eagle.model.ea_model import EaModel\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "BASE_ID  = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "EAGLE_ID = \"yuhuili/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B\"\n",
        "\n",
        "# Longer prompt improves EAGLE acceptance rate (more context = better drafts)\n",
        "PROMPT = (\n",
        "    \"Write a detailed explanation about how machine learning models work, \"\n",
        "    \"including the training process, inference, and optimization techniques.\"\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tokenizer\n",
        "# DeepSeek-R1-Distill uses LLaMA architecture, PreTrainedTokenizerFast works.\n",
        "# -----------------------------------------------------------------------------\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(BASE_ID, trust_remote_code=True)\n",
        "messages = [{\"role\": \"user\", \"content\": PROMPT}]\n",
        "chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(chat, return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[1]\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 1: BASELINE\n",
        "# Standard autoregressive generation - no speculative decoding.\n",
        "# =============================================================================\n",
        "clear_vram()\n",
        "print(\"\\n‚öôÔ∏è  Testing [1/2]: Baseline...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Warmup: ensures CUDA kernels are compiled before timing\n",
        "with torch.no_grad():\n",
        "    for _ in range(2):\n",
        "        _ = model.generate(inputs.input_ids, max_new_tokens=20, do_sample=False)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Timed run\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(inputs.input_ids, max_new_tokens=100, do_sample=False)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - t1\n",
        "\n",
        "tokens = out.shape[1] - input_len\n",
        "tps = tokens / elapsed\n",
        "results.append({\"Method\": \"Baseline\", \"TPS\": tps, \"Time\": elapsed,\n",
        "                \"Tokens\": tokens, \"Input\": input_len})\n",
        "print(f\"   Generated : {tokens} tokens in {elapsed:.2f}s = {tps:.2f} tok/s\")\n",
        "print(f\"   VRAM used : {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "del model\n",
        "clear_vram()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEST 2: EAGLE-3\n",
        "# Speculative decoding with EAGLE-3 multi-layer hidden state draft head.\n",
        "# use_eagle3=True enables the EAGLE-3 draft mechanism.\n",
        "# =============================================================================\n",
        "print(\"\\nü¶Ö Testing [2/2]: EAGLE-3...\")\n",
        "\n",
        "eagle = EaModel.from_pretrained(\n",
        "    base_model_path=BASE_ID,\n",
        "    ea_model_path=EAGLE_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_eagle3=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "eagle.eval()\n",
        "print(f\"   VRAM after load : {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "# Warmup\n",
        "with torch.no_grad():\n",
        "    for _ in range(2):\n",
        "        _ = eagle.eagenerate(inputs.input_ids, max_new_tokens=20)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Timed run\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "with torch.no_grad():\n",
        "    out = eagle.eagenerate(inputs.input_ids, max_new_tokens=100, temperature=0.5)\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - t1\n",
        "\n",
        "tokens = out.shape[1] - input_len\n",
        "tps = tokens / elapsed\n",
        "results.append({\"Method\": \"EAGLE-3\", \"TPS\": tps, \"Time\": elapsed,\n",
        "                \"Tokens\": tokens, \"Input\": input_len})\n",
        "print(f\"   Generated : {tokens} tokens in {elapsed:.2f}s = {tps:.2f} tok/s\")\n",
        "\n",
        "del eagle\n",
        "clear_vram()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RESULTS\n",
        "# =============================================================================\n",
        "df = pd.DataFrame(results)\n",
        "df['Speedup'] = df['TPS'] / df.iloc[0]['TPS']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä BENCHMARK RESULTS (100 tokens, longer prompt)\")\n",
        "print(\"=\" * 70)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\" * 70)\n",
        "\n",
        "baseline_tps = df.iloc[0]['TPS']\n",
        "eagle_tps    = df.iloc[1]['TPS']\n",
        "speedup      = df.iloc[1]['Speedup']\n",
        "time_saved   = df.iloc[0]['Time'] - df.iloc[1]['Time']\n",
        "\n",
        "print(f\"\\nüéØ Summary:\")\n",
        "print(f\"   Baseline : {baseline_tps:.2f} tok/s\")\n",
        "print(f\"   EAGLE-3  : {eagle_tps:.2f} tok/s\")\n",
        "print(f\"   Speedup  : {speedup:.2f}x\")\n",
        "print(f\"   Time saved: {time_saved:.2f}s\")\n",
        "\n",
        "if speedup >= 1.3:\n",
        "    print(\"\\n‚úÖ EXCELLENT: EAGLE-3 provides significant speedup\")\n",
        "elif speedup >= 1.15:\n",
        "    print(\"\\n‚úì  GOOD: EAGLE-3 provides moderate speedup\")\n",
        "elif speedup >= 1.05:\n",
        "    print(\"\\n‚ö†Ô∏è  MODEST: EAGLE-3 provides minimal speedup\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ISSUE: EAGLE-3 is slower than baseline\")\n",
        "    print(\"   Possible causes:\")\n",
        "    print(\"   - Prompt too short (EAGLE needs longer context)\")\n",
        "    print(\"   - Generation too short (overhead dominates)\")\n",
        "    print(\"   - Quantization degrading draft quality\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DIVERGENCE ANALYSIS\n",
        "# Derives Total Variation Distance (TVD) from the observed speedup.\n",
        "#\n",
        "# Theory:\n",
        "#   In speculative decoding, if the draft model accepts tokens with\n",
        "#   probability alpha, the expected accepted tokens per step follows:\n",
        "#       tau = (1 - alpha^(gamma+1)) / (1 - alpha)\n",
        "#   where gamma is the draft length (5 for EAGLE-3).\n",
        "#   TVD is then: TVD = 1 - alpha\n",
        "#   A lower TVD means the draft distribution closely matches the target.\n",
        "# =============================================================================\n",
        "gamma    = 5    # EAGLE-3 draft length (tokens proposed per step)\n",
        "overhead = 0.1  # EAGLE head forward pass overhead (~10% of target model)\n",
        "tau      = speedup * (1 + overhead)  # Estimated avg accepted tokens/step\n",
        "\n",
        "\n",
        "def estimate_alpha(target_tau, g, tolerance=0.001):\n",
        "    \"\"\"Binary search for acceptance rate alpha given observed tau.\"\"\"\n",
        "    low, high = 0.0, 1.0\n",
        "    for _ in range(20):\n",
        "        mid = (low + high) / 2\n",
        "        current_tau = (1 - mid**(g + 1)) / (1 - mid) if mid < 1.0 else g + 1\n",
        "        if abs(current_tau - target_tau) < tolerance:\n",
        "            return mid\n",
        "        if current_tau < target_tau:\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid\n",
        "    return low\n",
        "\n",
        "\n",
        "alpha = estimate_alpha(tau, gamma)\n",
        "tvd   = 1.0 - alpha\n",
        "\n",
        "print(\"\\nüìê Divergence Analysis:\")\n",
        "print(f\"   Avg Tokens Accepted/Step (œÑ) : {tau:.2f}\")\n",
        "print(f\"   Token Acceptance Rate (Œ±)    : {alpha*100:.1f}%\")\n",
        "print(f\"   Total Variation Distance (TVD): {tvd:.4f}\")\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "if tvd < 0.2:\n",
        "    print(f\"   EXCELLENT ALIGNMENT (TVD={tvd:.2f}): Draft head closely mirrors\")\n",
        "    print(f\"   the target distribution ‚Äî high acceptance drives the speedup.\")\n",
        "elif tvd < 0.4:\n",
        "    print(f\"   MODERATE ALIGNMENT (TVD={tvd:.2f}): Draft accepts most tokens\")\n",
        "    print(f\"   but diverges on less predictable outputs.\")\n",
        "else:\n",
        "    print(f\"   DIVERGENT (TVD={tvd:.2f}): Draft and target disagree frequently.\")\n",
        "    print(f\"   Consider a longer prompt or more generation steps.\")\n",
        "\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sd7Ix1vccgMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}